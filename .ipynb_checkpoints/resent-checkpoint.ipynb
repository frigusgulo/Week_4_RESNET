{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (9,9)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "torch.manual_seed(0)\n",
    "import scipy.signal as sp\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![simpleresnet.png](simpleresnet.png)\n",
    "\n",
    "This exercies uses a simple implementation of a deep neural network to explore the vanishing gradient problem\n",
    "\n",
    "We have provided an example of a very simple feed forward network that has a strongly vanishing gradient w.r.t. the weights that are increasingly close to the input end. \n",
    "\n",
    "Your job is to modify the network so that it uses skip connections and make observations on what happens to the gradient after. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an activation function\n",
    "activation = torch.tanh\n",
    "\n",
    "# Choose a number of iterations\n",
    "n = 500\n",
    "\n",
    "\n",
    "# Store the feed-forward steps\n",
    "w_list = []\n",
    "z_list = []\n",
    "a_list = []\n",
    "\n",
    "# Make up some data\n",
    "z_obs = torch.tensor([1.0])\n",
    "\n",
    "# Initial value\n",
    "x = torch.randn((1,),requires_grad=True)\n",
    "z_prev = x\n",
    "\n",
    "# Loop over a number of hidden layers\n",
    "for i in range(n):\n",
    "    # New weight\n",
    "    w_i = torch.tensor([1.0],requires_grad=True)\n",
    "\n",
    "    # Linear transform\n",
    "    a_i = z_prev*w_i\n",
    "\n",
    "    # Activation\n",
    "    zprime_i = activation(a_i)\n",
    "\n",
    "    #TODO: replace the line below with one that would add a skip connection\n",
    "    z_i = zprime_i\n",
    "    # Store forward model stuff\n",
    "    w_list.append(w_i)\n",
    "    z_list.append(z_prev)\n",
    " \n",
    "\n",
    "    # output of layer i becomes input for layer i+1\n",
    "    z_prev = z_i\n",
    "\n",
    "# Objective function\n",
    "L = torch.sqrt((z_i - z_obs)**2)\n",
    "\n",
    "# Reverse-mode AD\n",
    "L.backward()\n",
    "\n",
    "# Print each weight's gradient\n",
    "w_grad_init = []\n",
    "\n",
    "#w_list.reverse()\n",
    "for i in range(len(w_list)):\n",
    "    grad = torch.abs(w_list[i].grad).tolist()[0]\n",
    "    w_grad_init.append(grad)\n",
    "    \n",
    "plt.semilogy(w_grad_init)\n",
    "plt.title('Hidden Layer Gradients of a FNN Without Skip Connections')\n",
    "plt.xlabel('Hidden Layers')\n",
    "plt.ylabel('Weight Gradients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the feed-forward steps\n",
    "w_list = []\n",
    "z_list = []\n",
    "a_list = []\n",
    "\n",
    "# Make up some data\n",
    "z_obs = torch.randn((1,))\n",
    "# Initial value\n",
    "x = torch.randn((1,),requires_grad=True)\n",
    "z_prev = x\n",
    "# Loop over a number of hidden layers\n",
    "\n",
    "\n",
    "\n",
    "skip1 = #TODO\n",
    "skip2 = #TODO  \n",
    "\n",
    "    \n",
    "for i in range(1,n+1):\n",
    "    # New weight\n",
    "    w_i = torch.tensor([1.],requires_grad=True)\n",
    "   \n",
    "    # Linear transform\n",
    "    a_i = w_i*z_prev \n",
    "\n",
    "    # Activation\n",
    "    zprime_i = activation(a_i) \n",
    "\n",
    "    # TODO: replace the line below with one that would add a skip connection\n",
    "    \n",
    "    # use the .add(tensor) method\n",
    "    \n",
    "    # think about how we would skip multiple layers using MULTIPLE skip lengths in this instance (hint, use branching and the modulus operator)\n",
    "    \n",
    "    # We found the results to be VERY non-linear\n",
    " \n",
    "    \n",
    "    # Store forward model stuff\n",
    "    w_list.append(w_i)\n",
    "    z_list.append(z_prev)\n",
    "    a_list.append(a_i)\n",
    "    # output of layer i becomes input for layer i+1\n",
    "    z_prev = z_i\n",
    "\n",
    "# Objective function\n",
    "L = torch.sqrt((z_i - z_obs)**2)\n",
    "\n",
    "# Reverse-mode AD\n",
    "L.backward()\n",
    "\n",
    "# Print each weight's gradient\n",
    "w_grad = []\n",
    "\n",
    "#w_list.reverse()\n",
    "for i in range(len(w_list)):\n",
    "    grad = torch.abs(w_list[i].grad).tolist()[0]\n",
    "    w_grad.append(grad)\n",
    "\n",
    "#w_grad = sp.savgol_filter(w_grad,5,3)\n",
    "plt.semilogy(w_grad,label='Skip Connections')\n",
    "plt.semilogy(w_grad_init,labl='No Skip Connections') #compare to previous network\n",
    "plt.title('Hidden Layer Gradients of a FNN With Skip Connections')\n",
    "plt.xlabel('Hidden Layers')\n",
    "plt.ylabel('Weight Gradients')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how implementing skip connections seemingly solve the problem of vanishing gradients, we've learned all we can from the paper, lets look at some applications\n",
    "\n",
    "------------\n",
    "\n",
    "Below is a simple example of an image processing problem where vanishing gradient becomes an issue (no need to show it this time)\n",
    "\n",
    "For training and testing data I generated random images for a training and test set. If the small problems are too easy feel free to increase the size of the datasets to make for more challenging problems\n",
    "\n",
    "After you get done with the conceptual questions below, feel free to change the architecture of the below net. Make 3 changes to the architecture, record the loss difference after 100 iterations, and come up with a justification for that difference in loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run an implemenation of a simple deep convolutional network on MNIST (handwritten numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "train_data = train_dataset.data[::20].unsqueeze(1).float()\n",
    "train_targets = train_dataset.targets[::20].unsqueeze(1)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "test_data = test_dataset.data[::20].unsqueeze(1).float()\n",
    "test_targets = test_dataset.targets[::20].unsqueeze(1)\n",
    "\n",
    "training_data = TensorDataset(train_data,train_targets)\n",
    "testing_data = TensorDataset(test_data,test_targets)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data,\n",
    "                                          batch_size=100,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testing_data,\n",
    "                                          batch_size=100,\n",
    "                                          shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model allows you to input a specified number of convolutional layers in your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic net class\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_input_images, num_layers):\n",
    "        \n",
    "        # batch size is needed to configure \n",
    "        self.num_input_images = num_input_images\n",
    "        self.num_layers = num_layers\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, 3, padding = 1)\n",
    "        self.linearization2 = nn.Linear(5*26*26,10)\n",
    "        self.convout = nn.Conv2d(1, 5, 3)\n",
    "        self.linears = nn.ModuleList([nn.Linear(28,28)])\n",
    "        self.linears.extend([nn.Linear(28, 28) for i in range(1, self.num_layers-1)])\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        ### Hint, you can store the input x here\n",
    "        zprev = x\n",
    "        # convolution\n",
    "        self.conv1(zprev)\n",
    "        for i in range(1,self.num_layers-1): #loops over a set number of blocks conv to relu\n",
    "            \n",
    "            x = self.linears[i](zprev)\n",
    "            zi = F.relu(x)\n",
    "            zprev = zi\n",
    "    \n",
    "            \n",
    "        x = self.convout(zi)\n",
    "        # outputed images needed to be flattened for a linear layer\n",
    "        x = x.view(self.num_input_images, 5*26*26)\n",
    "        # find linear patterns in non-linear data\n",
    "        x = self.linearization2(x)\n",
    "        return x    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epic, your_net):\n",
    "        train_accuracy = []\n",
    "        test_accuracy = []\n",
    "        \n",
    "        model = your_net\n",
    "        model.to(device)\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss() #since ive set this up as a classification problem with bins number of classes\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "        epochs = epic\n",
    "        # Loop over the data\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "                model.train()\n",
    "                # Loop over each subset of data\n",
    "\n",
    "                correct = 0\n",
    "                total = 0        \n",
    "\n",
    "                for d,t in train_loader:\n",
    "                        # Zero out the optimizer's gradient buffer\n",
    "                        optimizer.zero_grad()\n",
    "                        # Make a prediction based on the model\n",
    "                        outputs = model(d)\n",
    "                        # Compute the loss\n",
    "                        loss = criterion(outputs,t[:,0])\n",
    "                        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "                        loss.backward()\n",
    "                        # Use the derivative information to update the parameters\n",
    "                        optimizer.step()\n",
    "\n",
    "                        \n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        correct += len(predicted[predicted==t[:,0]])\n",
    "                        total += len(predicted.flatten())\n",
    "                                \n",
    "                correcttest = 0\n",
    "                totaltest = 0        \n",
    "\n",
    "                for d,t in test_loader:\n",
    "                \n",
    "                    outputs = model(d)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    correcttest += len(predicted[predicted==t[:,0]])\n",
    "                    totaltest += len(predicted.flatten())\n",
    "                \n",
    "                if epoch%10==0:      \n",
    "                        print(epoch,loss.item(), ' Train_Accuracy = ', correct/total*100,' Test_Accuracy = ', correcttest/totaltest*100)\n",
    "                \n",
    "                train_accuracy.append(correct/total*100)\n",
    "                test_accuracy.append(correcttest/totaltest*100)\n",
    "        \n",
    "        return train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can mess with these paramenters and see how your model performance changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_images = 100\n",
    "num_classes = 10\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 50 #how long to run the model\n",
    "num_layers = 10 #how many convolutions to perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainacc_init, testacc_init = train_model(num_epochs, Net(num_input_images, num_layers)) #train the model for a hundred epochs\n",
    "\n",
    "plt.plot(trainacc_init, label = 'training accuracy')\n",
    "plt.plot(testacc_init, label = 'test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar fashion to what you did on the previous FNN, add skip connects and see if you can improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic net class\n",
    "skip1 = # TODO fill in SKIPS\n",
    "skip2 = #\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_input_images, num_layers):\n",
    "        \n",
    "        # batch size is needed to configure \n",
    "        self.num_input_images = num_input_images\n",
    "        self.num_layers = num_layers\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, 3, padding = 1)\n",
    "        #self.linearization1 = nn.Linear(28,28)\n",
    "        self.linearization2 = nn.Linear(5*26*26,10)\n",
    "        self.convout = nn.Conv2d(1, 5, 3)\n",
    "        self.linears = nn.ModuleList([nn.Linear(28,28)])\n",
    "        self.linears.extend([nn.Linear(28, 28) for i in range(1, self.num_layers-1)])\n",
    "        \n",
    "        self.a_list = []\n",
    "        self.z_list = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ### Hint, you can store the input x here\n",
    "        zprev = x\n",
    "        # convolution\n",
    "        self.conv1(zprev)\n",
    "        for i in range(1,self.num_layers-1): #loops over a set number of blocks conv to relu\n",
    "            \n",
    "            #ai = self.linearization(zprev)\n",
    "          \n",
    "            ai = self.linears[i](zprev)\n",
    "            \n",
    "            # TODO: Implement skip connections in a similar fashion to what was done previously.\n",
    "            # Think carefully about your skip intervals, and what should be used where\n",
    "            # hint: use the lists defined as a class object, the modulus operator, and the .add() method\n",
    "           \n",
    "            zi = F.relu(ai)\n",
    "            zprev = zi\n",
    "            self.z_list.append(zi)\n",
    "            self.a_list.append(ai)\n",
    "            \n",
    "            \n",
    "        x = self.convout(zi)\n",
    "        # outputed images needed to be flattened for a linear layer\n",
    "        x = x.view(self.num_input_images, 5*26*26)\n",
    "        # find linear patterns in non-linear data\n",
    "        x = self.linearization2(x)\n",
    "        return x    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainacc, testacc = train_model(num_epochs, ResNet(num_input_images, num_layers)) #train the model for a hundred epochs\n",
    "\n",
    "plt.plot(trainacc, label = 'training accuracy (skip connects)')\n",
    "plt.plot(testacc, label = 'test accuracy (skip connects)')\n",
    "plt.plot(trainacc_init, label = 'training accuracy (no skip connects)')\n",
    "plt.plot(testacc_init, label = 'test accuracy (no skip connects)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "1. What is the vanishing gradient problem, and what is its primary cause?\n",
    "\n",
    "2. What are 4 limitations to optimizing a deep convolutional neural network?\n",
    "\n",
    "3. In terms of how a given block of a network is \"fitted\", what is the key difference between using skip connections and traditional blocks?\n",
    "\n",
    "4. In the context of model hyper-parameters, what additional parameters is added in the res-net implementation?\n",
    "\n",
    "5. How do skip connections resolve the \"vanishing gradient\" problem? (Open Ended)\n",
    "\n",
    "6. Give an appropriate anology for how kernels are used to extract features from images (i.e. sanding wood)\n",
    "\n",
    "7. Was this a good paper when it was released? Is it a good paper now? What has changed between now and it's initial release point? What other methods are there of solving the vanishing gradient problem? (Open Ended)\n",
    "\n",
    "8. What interval of skip connections did you use and where were they applied to? Did you find any #accuracygainz ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
