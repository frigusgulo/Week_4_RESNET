{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (9,9)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![simpleresnet.png](simpleresnet.png)\n",
    "\n",
    "This exercies uses a simple implementation of a deep neural network to explore the vanishing gradient problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-0.]), tensor([-0.0727]), tensor([-0.1319]), tensor([-0.1892])]\n"
     ]
    }
   ],
   "source": [
    "# Choose an activation function\n",
    "activation = torch.tanh\n",
    "\n",
    "# Choose a number of iterations\n",
    "n = 4\n",
    "\n",
    "# Store the feed-forward steps\n",
    "w_list = []\n",
    "z_list = []\n",
    "a_list = []\n",
    "\n",
    "# Make up some data\n",
    "z_obs = torch.tensor([1.0])\n",
    "\n",
    "# Initial value\n",
    "x = torch.tensor([10.],requires_grad=True)\n",
    "z_prev = x\n",
    "\n",
    "# Loop over a number of hidden layers\n",
    "for i in range(n):\n",
    "    # New weight\n",
    "    w_i = torch.tensor([1.0],requires_grad=True)\n",
    "\n",
    "    # Linear transform\n",
    "    a_i = z_prev*w_i\n",
    "\n",
    "    # Activation\n",
    "    zprime_i = activation(a_i)\n",
    "\n",
    "    # Without skip connection\n",
    "    z_i = zprime_i\n",
    "\n",
    "    #z_i = zprime_i + z_prev\n",
    "\n",
    "    # Store forward model stuff\n",
    "    w_list.append(w_i)\n",
    "    z_list.append(zprime_i)\n",
    "    a_list.append(a_i)\n",
    "\n",
    "    # output of layer i becomes input for layer i+1\n",
    "    z_prev = z_i\n",
    "\n",
    "# Objective function\n",
    "L = 0.5*(z_i - z_obs)**2\n",
    "\n",
    "# Reverse-mode AD\n",
    "L.backward()\n",
    "\n",
    "# Print each weight's gradient\n",
    "print([w_.grad for w_ in w_list])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how implementing skip connections seemingly solve the problem of vanishing gradients, that's all the paper was saying\n",
    "<br>\n",
    "<br>\n",
    "This is a simple example of an image processing problem where adding a skipped connection would be helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic net class\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_input_images):\n",
    "        \n",
    "        # batch size is needed to configure \n",
    "        self.num_input_images = num_input_images\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 5, 3)\n",
    "        self.linearization = nn.Linear(5*26*26,10)        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # convolution\n",
    "        x = self.conv1(x)\n",
    "        # activation\n",
    "        x = F.relu(x)\n",
    "        # outputed images needed to be flattened for a linear layer\n",
    "        x = x.view(self.num_input_images, 5*26*26)\n",
    "        # find linear patterns in non-linear data\n",
    "        x = self.linearization(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_images = 100\n",
    "num_epochs = 100\n",
    "num_classes = 10\n",
    "# Everyone's playing with the same seed, same data\n",
    "torch.manual_seed(0)\n",
    "rand_train_data = torch.randn(num_input_images, 1, 28, 28)\n",
    "rand_test_data = torch.randn(num_input_images, 1, 28, 28)\n",
    "rand_labels = torch.LongTensor(num_input_images).random_(0, 10)\n",
    "\n",
    "learning_rate = 1e-3  # The speed of convergence\n",
    "\n",
    "# net class\n",
    "net = Net(num_input_images)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.334779739379883\n",
      "1 2.080829381942749\n",
      "2 1.861109733581543\n",
      "3 1.6222866773605347\n",
      "4 1.4008594751358032\n",
      "5 1.2021803855895996\n",
      "6 1.0175918340682983\n",
      "7 0.8464715480804443\n",
      "8 0.6945523619651794\n",
      "9 0.5652304291725159\n",
      "10 0.45722895860671997\n",
      "11 0.3678813874721527\n",
      "12 0.2950149476528168\n",
      "13 0.23656459152698517\n",
      "14 0.1902393400669098\n",
      "15 0.1537216305732727\n",
      "16 0.12497119605541229\n",
      "17 0.10233154892921448\n",
      "18 0.08448765426874161\n",
      "19 0.07039088010787964\n",
      "20 0.059205152094364166\n",
      "21 0.05027180165052414\n",
      "22 0.04308139905333519\n",
      "23 0.037246380001306534\n",
      "24 0.032474275678396225\n",
      "25 0.028543196618556976\n",
      "26 0.02528352662920952\n",
      "27 0.02256440930068493\n",
      "28 0.020283177495002747\n",
      "29 0.018358616158366203\n",
      "30 0.016725942492485046\n",
      "31 0.015333104878664017\n",
      "32 0.014138107188045979\n",
      "33 0.01310708001255989\n",
      "34 0.012212511152029037\n",
      "35 0.01143216248601675\n",
      "36 0.01074786577373743\n",
      "37 0.010144847445189953\n",
      "38 0.009610974229872227\n",
      "39 0.009136296808719635\n",
      "40 0.008712565526366234\n",
      "41 0.008332919329404831\n",
      "42 0.007991624064743519\n",
      "43 0.0076838284730911255\n",
      "44 0.007405398413538933\n",
      "45 0.007152827922254801\n",
      "46 0.006923077628016472\n",
      "47 0.006713528651744127\n",
      "48 0.006521852687001228\n",
      "49 0.006346080452203751\n",
      "50 0.006184424739331007\n",
      "51 0.006035310681909323\n",
      "52 0.0058973790146410465\n",
      "53 0.005769452545791864\n",
      "54 0.005650399252772331\n",
      "55 0.005539321340620518\n",
      "56 0.005435372702777386\n",
      "57 0.0053377882577478886\n",
      "58 0.0052459524013102055\n",
      "59 0.005159277934581041\n",
      "60 0.0050772749818861485\n",
      "61 0.004999479278922081\n",
      "62 0.004925511311739683\n",
      "63 0.004855020437389612\n",
      "64 0.0047877030447125435\n",
      "65 0.004723305348306894\n",
      "66 0.004661570768803358\n",
      "67 0.004602275788784027\n",
      "68 0.0045452057383954525\n",
      "69 0.00449027307331562\n",
      "70 0.004437228664755821\n",
      "71 0.004385963547974825\n",
      "72 0.004336359910666943\n",
      "73 0.004288282711058855\n",
      "74 0.004241610411554575\n",
      "75 0.004196294583380222\n",
      "76 0.004152179230004549\n",
      "77 0.0041091907769441605\n",
      "78 0.004067308735102415\n",
      "79 0.00402639526873827\n",
      "80 0.003986412659287453\n",
      "81 0.003947332035750151\n",
      "82 0.003909055143594742\n",
      "83 0.003871534951031208\n",
      "84 0.0038347572553902864\n",
      "85 0.003798646852374077\n",
      "86 0.00376320187933743\n",
      "87 0.0037283729761838913\n",
      "88 0.0036941361613571644\n",
      "89 0.003660459304228425\n",
      "90 0.0036273289006203413\n",
      "91 0.0035947090946137905\n",
      "92 0.0035625810269266367\n",
      "93 0.003530932357534766\n",
      "94 0.0034997451584786177\n",
      "95 0.0034689942840486765\n",
      "96 0.003438695101067424\n",
      "97 0.003408807562664151\n",
      "98 0.0033793083857744932\n",
      "99 0.0033502120058983564\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad() # Intialize the hidden weight to all zeros\n",
    "    outputs = net(rand_input) # Forward pass: compute the output class given a image\n",
    "    loss = criterion(outputs, rand_labels) # Compute the loss: difference between the output class and the pre-given label\n",
    "    loss.backward() # Backward pass: compute the weight\n",
    "    optimizer.step()\n",
    "    num_batches += 1\n",
    "    print(epoch, loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "1. What is the vanishing gradient problem, and what is its primary cause?\n",
    "\n",
    "2. What are 4 limitations to optimizing a deep convolutional neural network?\n",
    "\n",
    "3. In terms of how a given block of a network is \"fitted\", what is the key difference between using skip connections and traditional blocks?\n",
    "\n",
    "4. In the context of model hyper-parameters, what additional parameters is added in the res-net implementation?\n",
    "\n",
    "5. How do skip connections resolve the \"vanishing gradient\" problem?\n",
    "\n",
    "6. Give an appropriate anology for how kernels are used to extract features from images (i.e. sanding wood)\n",
    "\n",
    "7. max's questions: was this a good paper when it was released? Is it a good paper now? What has changed between now and it's initial release point? What other methods are there of solving the vanishing gradient problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
