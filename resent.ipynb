{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (9,9)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![simpleresnet.png](simpleresnet.png)\n",
    "\n",
    "This exercies uses a simple implementation of a deep neural network to explore the vanishing gradient problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-0.]), tensor([-0.0727]), tensor([-0.1319]), tensor([-0.1892])]\n"
     ]
    }
   ],
   "source": [
    "# Choose an activation function\n",
    "activation = torch.tanh\n",
    "\n",
    "# Choose a number of iterations\n",
    "n = 4\n",
    "\n",
    "# Store the feed-forward steps\n",
    "w_list = []\n",
    "z_list = []\n",
    "a_list = []\n",
    "\n",
    "# Make up some data\n",
    "z_obs = torch.tensor([1.0])\n",
    "\n",
    "# Initial value\n",
    "x = torch.tensor([10.],requires_grad=True)\n",
    "z_prev = x\n",
    "\n",
    "# Loop over a number of hidden layers\n",
    "for i in range(n):\n",
    "    # New weight\n",
    "    w_i = torch.tensor([1.0],requires_grad=True)\n",
    "\n",
    "    # Linear transform\n",
    "    a_i = z_prev*w_i\n",
    "\n",
    "    # Activation\n",
    "    zprime_i = activation(a_i)\n",
    "\n",
    "    #TODO: replace the line below with one that would add a skip connection\n",
    "    z_i = zprime_i\n",
    "    \n",
    "    # Store forward model stuff\n",
    "    w_list.append(w_i)\n",
    "    z_list.append(zprime_i)\n",
    "    a_list.append(a_i)\n",
    "\n",
    "    # output of layer i becomes input for layer i+1\n",
    "    z_prev = z_i\n",
    "\n",
    "# Objective function\n",
    "L = 0.5*(z_i - z_obs)**2\n",
    "\n",
    "# Reverse-mode AD\n",
    "L.backward()\n",
    "\n",
    "# Print each weight's gradient\n",
    "print([w_.grad for w_ in w_list])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how implementing skip connections seemingly solve the problem of vanishing gradients, we've learned all we can from the paper, lets look at some applications\n",
    "\n",
    "------------\n",
    "\n",
    "Below is a simple example of an image processing problem where vanishing gradient becomes an issue (no need to show it this time)\n",
    "\n",
    "For training and testing data I generated random images for a training and test set. If the small problems are too easy feel free to increase the size of the datasets to make for more challenging problems\n",
    "\n",
    "After you get done with the conceptual questions below, feel free to change the architecture of the below net. Make 3 changes to the architecture, record the loss differnece after 100 iterations, and come up with a justification for that difference in loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic net class\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_input_images):\n",
    "        \n",
    "        # batch size is needed to configure \n",
    "        self.num_input_images = num_input_images\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 5, 3)\n",
    "        self.linearization = nn.Linear(5*26*26,10)        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # convolution\n",
    "        x = self.conv1(x)\n",
    "        # activation\n",
    "        x = F.relu(x)\n",
    "        # outputed images needed to be flattened for a linear layer\n",
    "        x = x.view(self.num_input_images, 5*26*26)\n",
    "        # find linear patterns in non-linear data\n",
    "        x = self.linearization(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_images = 100\n",
    "num_epochs = 20\n",
    "num_classes = 10\n",
    "\n",
    "# Everyone's playing with the same seed, same data\n",
    "torch.manual_seed(0)\n",
    "rand_train_data = torch.randn(num_input_images, 1, 28, 28)\n",
    "rand_train_labels = torch.LongTensor(num_input_images).random_(0, 10)\n",
    "rand_test_data = torch.randn(num_input_images, 1, 28, 28)\n",
    "rand_test_labels = torch.LongTensor(num_input_images).random_(0, 10)\n",
    "\n",
    "learning_rate = 1e-3  # The speed of convergence\n",
    "\n",
    "# net class\n",
    "net = Net(num_input_images)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7982895374298096\n",
      "2.7994678020477295\n",
      "2.8006298542022705\n",
      "2.8017754554748535\n",
      "2.8029046058654785\n",
      "2.804018974304199\n",
      "2.8051183223724365\n",
      "2.8062021732330322\n",
      "2.807271957397461\n",
      "2.808326005935669\n",
      "2.809366464614868\n",
      "2.8103935718536377\n",
      "2.811405658721924\n",
      "2.8124053478240967\n",
      "2.8133926391601562\n",
      "2.8143680095672607\n",
      "2.8153305053710938\n",
      "2.8162829875946045\n",
      "2.8172245025634766\n",
      "2.8181543350219727\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad() # Intialize the hidden weight to all zeros\n",
    "    outputs = net(rand_train_data) # Forward pass: compute the output class given a image\n",
    "    loss = criterion(outputs, rand_train_labels) # Compute the loss: difference between the output class and the pre-given label\n",
    "    loss.backward() # Backward pass: compute the weight\n",
    "    optimizer.step()\n",
    "    test_output = net(rand_test_data)\n",
    "    loss = criterion(test_output, rand_test_labels)\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "1. What is the vanishing gradient problem, and what is its primary cause?\n",
    "\n",
    "2. What are 4 limitations to optimizing a deep convolutional neural network?\n",
    "\n",
    "3. In terms of how a given block of a network is \"fitted\", what is the key difference between using skip connections and traditional blocks?\n",
    "\n",
    "4. In the context of model hyper-parameters, what additional parameters is added in the res-net implementation?\n",
    "\n",
    "5. How do skip connections resolve the \"vanishing gradient\" problem?\n",
    "\n",
    "6. Give an appropriate anology for how kernels are used to extract features from images (i.e. sanding wood)\n",
    "\n",
    "7. max's questions: was this a good paper when it was released? Is it a good paper now? What has changed between now and it's initial release point? What other methods are there of solving the vanishing gradient problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
