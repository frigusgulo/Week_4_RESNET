{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (9,9)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "np.random.seed(0)\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![simpleresnet.png](simpleresnet.png)\n",
    "\n",
    "This exercies uses a simple implementation of a deep neural network to explore the vanishing gradient problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-0.]), tensor([-0.0727]), tensor([-0.1319]), tensor([-0.1892])]\n"
     ]
    }
   ],
   "source": [
    "# Choose an activation function\n",
    "activation = torch.tanh\n",
    "\n",
    "# Choose a number of iterations\n",
    "n = 4\n",
    "\n",
    "# Store the feed-forward steps\n",
    "w_list = []\n",
    "z_list = []\n",
    "a_list = []\n",
    "\n",
    "# Make up some data\n",
    "z_obs = torch.tensor([1.0])\n",
    "\n",
    "# Initial value\n",
    "x = torch.tensor([10.],requires_grad=True)\n",
    "z_prev = x\n",
    "\n",
    "# Loop over a number of hidden layers\n",
    "for i in range(n):\n",
    "    # New weight\n",
    "    w_i = torch.tensor([1.0],requires_grad=True)\n",
    "\n",
    "    # Linear transform\n",
    "    a_i = z_prev*w_i\n",
    "\n",
    "    # Activation\n",
    "    zprime_i = activation(a_i)\n",
    "\n",
    "    #TODO: replace the line below with one that would add a skip connection\n",
    "    z_i = zprime_i\n",
    "    \n",
    "    # Store forward model stuff\n",
    "    w_list.append(w_i)\n",
    "    z_list.append(zprime_i)\n",
    "    a_list.append(a_i)\n",
    "\n",
    "    # output of layer i becomes input for layer i+1\n",
    "    z_prev = z_i\n",
    "\n",
    "# Objective function\n",
    "L = 0.5*(z_i - z_obs)**2\n",
    "\n",
    "# Reverse-mode AD\n",
    "L.backward()\n",
    "\n",
    "# Print each weight's gradient\n",
    "print([w_.grad for w_ in w_list])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how implementing skip connections seemingly solve the problem of vanishing gradients, we've learned all we can from the paper, lets look at some applications\n",
    "\n",
    "------------\n",
    "\n",
    "Below is a simple example of an image processing problem where vanishing gradient becomes an issue (no need to show it this time)\n",
    "\n",
    "For training and testing data I generated random images for a training and test set. If the small problems are too easy feel free to increase the size of the datasets to make for more challenging problems\n",
    "\n",
    "After you get done with the conceptual questions below, feel free to change the architecture of the below net. Make 3 changes to the architecture, record the loss differnece after 100 iterations, and come up with a justification for that difference in loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "train_data = train_dataset.data[::20].unsqueeze(1).float()\n",
    "train_targets = train_dataset.targets[::20].unsqueeze(1)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "test_data = test_dataset.data[::20].unsqueeze(1).float()\n",
    "test_targets = test_dataset.targets[::20].unsqueeze(1)\n",
    "\n",
    "training_data = TensorDataset(train_data,train_targets)\n",
    "testing_data = TensorDataset(test_data,test_targets)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data,\n",
    "                                          batch_size=100,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testing_data,\n",
    "                                          batch_size=100,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.data.shape\n",
    "train_dataset.data[::20].shape\n",
    "train_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic net class\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_input_images, num_layers):\n",
    "        \n",
    "        # batch size is needed to configure \n",
    "        self.num_input_images = num_input_images\n",
    "        self.num_layers = num_layers\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, 3, padding = 1)\n",
    "        self.linearization = nn.Linear(5*26*26,10)        \n",
    "        self.convout = nn.Conv2d(1, 5, 3)\n",
    "    def forward(self, x):\n",
    "        zprev = x\n",
    "        # convolution\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.conv1(x)\n",
    "            # activation\n",
    "            x = F.relu(x)\n",
    "        x = self.convout(x)\n",
    "        # outputed images needed to be flattened for a linear layer\n",
    "        x = x.view(self.num_input_images, 5*26*26)\n",
    "        # find linear patterns in non-linear data\n",
    "        x = self.linearization(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_images = 100\n",
    "num_epochs = 1500\n",
    "num_classes = 10\n",
    "num_layers = 5\n",
    "# Everyone's playing with the same seed, same data\n",
    "torch.manual_seed(0)\n",
    "rand_train_data = torch.randn(num_input_images, 1, 28, 28)\n",
    "rand_train_labels = torch.LongTensor(num_input_images).random_(0, 10)\n",
    "rand_test_data = torch.randn(num_input_images, 1, 28, 28)\n",
    "rand_test_labels = torch.LongTensor(num_input_images).random_(0, 10)\n",
    "\n",
    "learning_rate = 1e-3  # The speed of convergence\n",
    "\n",
    "# net class\n",
    "net = Net(num_input_images, num_layers)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epic):\n",
    "\n",
    "        model = Net(num_input_images, num_layers)\n",
    "        model.to(device)\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss() #since ive set this up as a classification problem with bins number of classes\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "        epochs = epic\n",
    "        # Loop over the data\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "                model.train()\n",
    "                # Loop over each subset of data\n",
    "\n",
    "                correct = 0\n",
    "                total = 0        \n",
    "\n",
    "                for d,t in train_loader:\n",
    "                        # Zero out the optimizer's gradient buffer\n",
    "                        optimizer.zero_grad()\n",
    "                        # Make a prediction based on the model\n",
    "                        outputs = model(d)\n",
    "                        # Compute the loss\n",
    "                        loss = criterion(outputs,t[:,0])\n",
    "                        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "                        loss.backward()\n",
    "                        # Use the derivative information to update the parameters\n",
    "                        optimizer.step()\n",
    "\n",
    "                        if epoch%10==0: #every once in a while see how the model is doing\n",
    "                                _, predicted = torch.max(outputs.data, 1)\n",
    "                                correct += len(predicted[predicted==t[:,0]])\n",
    "                                total += len(predicted.flatten())\n",
    "                \n",
    "                if epoch%10==0:      \n",
    "                        print(epoch,loss.item(), 'Accuracy = ', correct/total*100)\n",
    "\n",
    "        return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.6370714902877808 Accuracy =  31.6\n",
      "10 0.3135767877101898 Accuracy =  88.66666666666667\n",
      "20 0.2120419293642044 Accuracy =  92.43333333333334\n",
      "30 0.17267699539661407 Accuracy =  95.66666666666667\n",
      "40 0.06949880719184875 Accuracy =  97.26666666666667\n",
      "50 0.11266805976629257 Accuracy =  98.76666666666667\n",
      "60 0.06865020096302032 Accuracy =  98.93333333333332\n",
      "70 0.017717797309160233 Accuracy =  99.83333333333333\n",
      "80 0.015102163888514042 Accuracy =  100.0\n",
      "90 0.005821489728987217 Accuracy =  100.0\n",
      "Test Accuracy =  80.60000000000001\n"
     ]
    }
   ],
   "source": [
    "net = train_model(100) #train the model for a hundred epochs\n",
    "\n",
    "correct = 0\n",
    "total = 0        \n",
    "\n",
    "for d,t in test_loader:\n",
    "    # Zero out the optimizer's gradient buffer\n",
    "    optimizer.zero_grad()\n",
    "    # Make a prediction based on the model\n",
    "    outputs = net(d)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    correct += len(predicted[predicted==t[:,0]])\n",
    "    total += len(predicted.flatten())\n",
    "                     \n",
    "print('Test Accuracy = ', correct/total*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "1. What is the vanishing gradient problem, and what is its primary cause?\n",
    "\n",
    "2. What are 4 limitations to optimizing a deep convolutional neural network?\n",
    "\n",
    "3. In terms of how a given block of a network is \"fitted\", what is the key difference between using skip connections and traditional blocks?\n",
    "\n",
    "4. In the context of model hyper-parameters, what additional parameters is added in the res-net implementation?\n",
    "\n",
    "5. How do skip connections resolve the \"vanishing gradient\" problem?\n",
    "\n",
    "6. Give an appropriate anology for how kernels are used to extract features from images (i.e. sanding wood)\n",
    "\n",
    "7. max's questions: was this a good paper when it was released? Is it a good paper now? What has changed between now and it's initial release point? What other methods are there of solving the vanishing gradient problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
